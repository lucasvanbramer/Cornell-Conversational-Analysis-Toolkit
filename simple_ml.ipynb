{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gfsLARjCnw5"
   },
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91ImBZ1aCuio"
   },
   "outputs": [],
   "source": [
    "from convokit import Corpus, download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mPpbvVV5Czye",
    "outputId": "2fed8c5a-2326-4a16-f549-13e14c686b4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/nianyiwang/.convokit/downloads/subreddit-de\n",
      "Dataset already exists at /Users/nianyiwang/.convokit/downloads/subreddit-germany\n"
     ]
    }
   ],
   "source": [
    "de = Corpus(filename=download(\"subreddit-de\"))\n",
    "germany = Corpus(filename=download(\"subreddit-germany\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting common users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_users = set()\n",
    "de_users=set()\n",
    "for usr in de.iter_users():\n",
    "    de_users.add(usr.name)\n",
    "for usr in germany.iter_users():\n",
    "    eng_users.add(usr.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14072\n",
      "43550\n"
     ]
    }
   ],
   "source": [
    "common_users = list(eng_users.intersection(de_users))\n",
    "print(len(common_users))\n",
    "eng_only_users = list(eng_users.difference(de_users))\n",
    "print(len(eng_only_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IamVasi'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_users[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract texts that bilingual users posted in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_conv = {}\n",
    "for utt in germany.iter_utterances():\n",
    "    usr = utt.user.name\n",
    "    if usr not in common_users or usr == '[deleted]':\n",
    "        continue\n",
    "    if usr not in user_conv and utt.text != '':\n",
    "        user_conv[usr] = [utt.text]\n",
    "    elif usr in user_conv and utt.text != '':\n",
    "        user_conv[usr].append(utt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Come of you please give us some context?', 'It actually is, the lower plaque below the old ASU sticker is gone.', \"True that - nothing really wants to kill you in German Forests... just make sure to follow any posted warnings for tree felling and you should be good.\\nOh yeah and don't just eat any mushroom! Enjoy!\", 'G’socks\\ngrattler', 'I heard Congstar is great for such needs.', 'I would also add your education to this. If you are a hairdresser and make six figures with your own salon big props - if you are a automotive engineer in Munich or Stuttgart and are living in your parents basement at the age of 30 because you can’t afford to move out .. well then there is something off.']\n",
      "13621\n"
     ]
    }
   ],
   "source": [
    "print(user_conv['panda527'])\n",
    "print(len(user_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing training set\n",
    "12000 bilingual users and 12000 no cross-over users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "bi_users = list(user_conv.keys())\n",
    "random.shuffle(bi_users)\n",
    "bi_users = bi_users[:12000]\n",
    "bi_utt = {}\n",
    "for usr in bi_users:\n",
    "    bi_utt[usr] = user_conv[usr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This part can be loaded from sin_utt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin_utts = {}\n",
    "# for utt in germany.iter_utterances():\n",
    "#     usr = utt.user.name\n",
    "#     if usr not in eng_only_users[:14000] or usr == '[deleted]':\n",
    "#         continue\n",
    "#     if usr not in sin_utts and utt.text != '':\n",
    "#         sin_utts[usr] = [utt.text]\n",
    "#     elif usr in sin_utts and utt.text != '':\n",
    "#         sin_utts[usr].append(utt.text)\n",
    "# sin_users = list(sin_utts.keys())\n",
    "# random.shuffle(sin_users)\n",
    "# sin_users = sin_users[:12000]\n",
    "# sin_utt = {}\n",
    "# for usr in sin_users:\n",
    "#     sin_utt[usr] = sin_utts[usr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dumping data to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('bi_utt.json', 'w') as outfile:\n",
    "    json.dump(bi_utt, outfile)\n",
    "# with open('sin_utt.json', 'w') as outfile:\n",
    "#     json.dump(sin_utt, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What do blondes and spaghetti have in common? They both wiggle when you eat them.', 'What do blondes and spaghetti have in common? They both wiggle when you eat them.', '(╯ಠ_ಠ）╯︵ ┻━┻', 'Did you hear about the Italian chef that died? He pasta way. ']\n",
      "[\"The comments are a madness of people saying all kinds of xenophobe, Islamophobe and racist insults fighting with others defending Muslims and refugees. The only proof we have that he was a refugee is that the guy who uploaded the video put that in the title as nobody understands what they're saying. I want to know whether this is real news or bullshit to try to finish all that stupidity. Thanks in advance from a Spanish fellow!\\n\\nEdit: I mean, of course the video itself is true. What I meant to ask is if it's true that the perpetrator is a refugee.\", \"If you were trying to be funny, you didn't achieve it. Don't joke about such things.\", \"Thanks. I don't get why are there so many people saying things like what Livohka says\", 'So, who is he?  Has he given any explanation at all?']\n"
     ]
    }
   ],
   "source": [
    "print(bi_utt['I_Like_Spaghetti'])\n",
    "print(sin_utt['wxsted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TF-IDF Matrix and applying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "bi_utt={}\n",
    "with open('bi_utt.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for usr in data:\n",
    "        bi_utt[usr] = data[usr]\n",
    "\n",
    "sin_utt={}\n",
    "with open('sin_utt.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for usr in data:\n",
    "        sin_utt[usr] = data[usr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [19:51<00:00, 10.07it/s]  \n",
      "100%|██████████| 12000/12000 [03:50<00:00, 52.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done preparing data lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "texts = []\n",
    "lengths = []\n",
    "labels = []\n",
    "poses = []\n",
    "i = 0\n",
    "for usr in tqdm(bi_utt):\n",
    "    labels.append(1)\n",
    "    new_text= ' '.join(bi_utt[usr])\n",
    "    texts.append(new_text)\n",
    "    lengths.append(len(new_text))\n",
    "    parts_of_speech = ' '.join(list(map(lambda x: x[1], nltk.pos_tag(nltk.word_tokenize(new_text)))))\n",
    "    poses.append(parts_of_speech)\n",
    "        \n",
    "for usr in tqdm(sin_utt):\n",
    "    labels.append(0)\n",
    "    new_text= ' '.join(sin_utt[usr])\n",
    "    texts.append(new_text)\n",
    "    lengths.append(len(new_text))\n",
    "    parts_of_speech = ' '.join(list(map(lambda x: x[1], nltk.pos_tag(nltk.word_tokenize(new_text)))))\n",
    "    poses.append(parts_of_speech)\n",
    "    \n",
    "print(\"done preparing data lists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 12000\n"
     ]
    }
   ],
   "source": [
    "print(labels.count(0), labels.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "pos_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = vectorizer.fit_transform(texts)\n",
    "pos_tfidf = pos_vectorizer.fit_transform(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1389]\n",
      " [  493]\n",
      " [11492]\n",
      " ...\n",
      " [ 1097]\n",
      " [  524]\n",
      " [  873]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(np.asarray(lengths).reshape(-1, 1))\n",
    "\n",
    "import numpy as np\n",
    "xtrain, xtest, ytrain, ytest = train_test_split( \n",
    "        tfidf, labels, test_size = 0.25, random_state = 0) \n",
    "\n",
    "lenxtrain, lenxtest, lenytrain, lenytest = train_test_split(\n",
    "        np.asarray(lengths).reshape(-1, 1), labels, test_size=0.25, random_state=0)\n",
    "\n",
    "posxtrain, posxtest, posytrain, posytest = train_test_split( \n",
    "        pos_tfidf, labels, test_size = 0.25, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF classifier trained.\n",
      "Length classifier trained.\n",
      "POS classifier trained.\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial').fit(xtrain, ytrain)\n",
    "print(\"TF-IDF classifier trained.\")\n",
    "len_clf = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial').fit(lenxtrain, lenytrain)\n",
    "print(\"Length classifier trained.\")\n",
    "pos_clf = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial').fit(posxtrain, posytrain)\n",
    "print(\"POS classifier trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "y_pred = clf.predict(xtest) \n",
    "leny_pred = len_clf.predict(lenxtest)\n",
    "posy_pred = pos_clf.predict(posxtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy          : 0.5\n",
      "Length classifier accuracy : 0.5851666666666666\n",
      "TF-IDF classifier accuracy : 0.7026666666666667\n",
      "POS classifier accuracy    : 0.637\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline accuracy          : 0.5\")\n",
    "print(\"Length classifier accuracy :\", accuracy_score(lenytest, leny_pred))\n",
    "print(\"TF-IDF classifier accuracy :\", accuracy_score(ytest, y_pred)) \n",
    "print(\"POS classifier accuracy    :\", accuracy_score(posytest, posy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[2134  825]\n",
      " [ 959 2082]]\n",
      "Normalized confusion matrix\n",
      "[[0.72 0.28]\n",
      " [0.32 0.68]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = [0,1]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        if i % 2 == 0 : \n",
    "            pos = i+0.25 \n",
    "        else: pos = i-0.25\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, pos, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "class_names=[0,1]\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(ytest, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(ytest, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3041\n",
      "2959\n"
     ]
    }
   ],
   "source": [
    "print(ytest.count(1))\n",
    "print(ytest.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
